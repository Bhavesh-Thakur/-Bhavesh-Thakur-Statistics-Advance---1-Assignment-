{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Explain the properties of the F-distribution.**\n",
        "\n",
        "## Properties of the F-Distribution\n",
        "\n",
        "The F-distribution is a continuous probability distribution that arises frequently in the context of variance analysis, particularly in **ANOVA** (Analysis of Variance) and **F-tests**. Some key properties of the F-distribution are:\n",
        "\n",
        "1. **Non-Negativity**: The F-distribution is always non-negative, meaning it only takes values greater than or equal to zero. This is because it is based on the ratio of two variances (which are always non-negative).\n",
        "\n",
        "2. **Asymmetry**: The F-distribution is skewed to the right (positively skewed). As the degrees of freedom increase, the distribution becomes less skewed and more symmetric.\n",
        "\n",
        "3. **Degrees of Freedom**: The shape of the F-distribution depends on two different degrees of freedom:\n",
        "   - The degrees of freedom in the numerator (df₁).\n",
        "   - The degrees of freedom in the denominator (df₂).\n",
        "   A larger number of degrees of freedom tends to make the distribution more symmetric and centered closer to 1.\n",
        "\n",
        "4. **F-Value at 1**: For equal degrees of freedom, the most probable value of the F-distribution is close to 1. This is because the F-distribution compares the ratio of two variances, and variances that are equal will result in an F-ratio of 1.\n",
        "\n",
        "5. **Right-Tailed**: The F-distribution is primarily used in right-tailed tests because it is designed to compare variances. Large values of F indicate that the variances are different, while values closer to 1 indicate similarity.\n",
        "\n",
        "6. **Dependent on Variances**: The F-distribution is used to compare the variances of two populations or test hypotheses about them. It is a ratio of two scaled chi-squared distributions.\n",
        "\n",
        "7. **Mean and Variance**:\n",
        "   - The mean of the F-distribution is approximately 1 when both the degrees of freedom are large.\n",
        "   - The variance of the F-distribution depends on both the numerator and denominator degrees of freedom.\n",
        "\n",
        "### Formula for F-statistic\n",
        "\n",
        "$$\n",
        "F = \\frac{\\frac{S_1^2}{d_1}}{\\frac{S_2^2}{d_2}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\(S_1^2\\) is the variance of the first sample.\n",
        "- \\(S_2^2\\) is the variance of the second sample.\n",
        "- \\(d_1\\) is the degrees of freedom for the numerator.\n",
        "- \\(d_2\\) is the degrees of freedom for the denominator.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4K1jo26uiV9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?**\n",
        "\n",
        " ## Uses of the F-Distribution in Statistical Tests\n",
        "\n",
        "The F-distribution is commonly used in the following statistical tests:\n",
        "\n",
        "### 1. **ANOVA (Analysis of Variance)**:\n",
        "   - The F-distribution is primarily used in ANOVA to compare the variances between groups.\n",
        "   - ANOVA is used when you want to test if the means of three or more groups are equal.\n",
        "   - The F-test in ANOVA checks if the variance between group means is significantly greater than the variance within the groups.\n",
        "\n",
        "### 2. **Regression Analysis**:\n",
        "   - In regression analysis, the F-distribution is used to test the overall significance of the model.\n",
        "   - It checks if the variation explained by the independent variables is significantly greater than the unexplained variation (residuals).\n",
        "   - The F-test in regression is used to test if at least one predictor variable has a significant relationship with the outcome variable.\n",
        "\n",
        "### 3. **F-Test for Equality of Variances**:\n",
        "   - The F-distribution is also used in an F-test to compare the variances of two populations.\n",
        "   - This test is appropriate when you want to test the assumption of equal variances in two populations (often used as a preliminary test before conducting t-tests or ANOVA).\n",
        "\n",
        "### Why the F-Distribution is Appropriate:\n",
        "   - The F-distribution is used in these tests because it is based on the ratio of two variances. The distribution accounts for both the sample size and the number of groups, making it suitable for comparing variances.\n",
        "   - It is right-skewed, allowing for the identification of significant differences when the ratio of variances is large.\n"
      ],
      "metadata": {
        "id": "D1cbYGe9kiwt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. What are the key assumptions required for conducting an F-test to compare the variances of two populations?**\n",
        "\n",
        "## Key Assumptions for Conducting an F-Test to Compare Variances\n",
        "\n",
        "When conducting an F-test to compare the variances of two populations, the following key assumptions must be met:\n",
        "\n",
        "### 1. **Normality of the Populations**:\n",
        "   - Both populations being compared must follow a normal distribution. The F-test is sensitive to deviations from normality, and violations of this assumption can lead to inaccurate results.\n",
        "\n",
        "### 2. **Independence of Samples**:\n",
        "   - The two samples must be independent of each other. This means that the observations in one sample should not influence or be related to the observations in the other sample.\n",
        "\n",
        "### 3. **Random Sampling**:\n",
        "   - The data must be collected through random sampling to ensure that the sample accurately represents the population. This helps in avoiding bias and ensuring the validity of the test results.\n",
        "\n",
        "### 4. **Measurement on a Continuous Scale**:\n",
        "   - The data should be measured on a continuous scale (interval or ratio). This is important because the F-test compares variances, which are meaningful only for continuous data.\n",
        "\n",
        "### 5. **Non-Negative Variances**:\n",
        "   - Both populations must have non-negative variances, as the F-distribution requires comparing positive variance values. A variance of zero would make the test invalid.\n",
        "\n",
        "### Note:\n",
        "- If any of these assumptions are violated, alternative methods (such as non-parametric tests) may be more appropriate for comparing variances.\n"
      ],
      "metadata": {
        "id": "25NI9_XXlvrK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. What is the purpose of ANOVA, and how does it differ from a t-test?**\n",
        "\n",
        "## Purpose of ANOVA and How It Differs from a t-Test\n",
        "\n",
        "### **Purpose of ANOVA (Analysis of Variance)**:\n",
        "   - **ANOVA** is used to compare the means of three or more groups to determine if there are statistically significant differences between them.\n",
        "   - It helps in analyzing the effect of one or more categorical independent variables on a continuous dependent variable.\n",
        "   - **Main Idea**: ANOVA compares the variance within groups to the variance between groups. If the between-group variance is significantly larger, it suggests that at least one group mean is different from the others.\n",
        "\n",
        "### **Difference Between ANOVA and t-Test**:\n",
        "\n",
        "1. **Number of Groups Compared**:\n",
        "   - **t-Test**: Used to compare the means of **two groups**.\n",
        "   - **ANOVA**: Used to compare the means of **three or more groups**. It avoids the need for multiple t-tests, which could increase the risk of Type I error (false positives).\n",
        "\n",
        "2. **Hypotheses**:\n",
        "   - **t-Test**: Tests if the means of two groups are equal.\n",
        "     - **Null Hypothesis (\\(H_0\\))**: The means of the two groups are equal.\n",
        "     - **Alternative Hypothesis (\\(H_1\\))**: The means of the two groups are not equal.\n",
        "   - **ANOVA**: Tests if the means of three or more groups are equal.\n",
        "     - **Null Hypothesis (\\(H_0\\))**: All group means are equal.\n",
        "     - **Alternative Hypothesis (\\(H_1\\))**: At least one group mean is different from the others.\n",
        "\n",
        "3. **Interpretation**:\n",
        "   - **t-Test**: Directly provides whether the difference between two means is significant.\n",
        "   - **ANOVA**: Indicates whether there are significant differences among multiple groups but does not specify which groups are different. Post-hoc tests (e.g., Tukey’s test) are required to identify specific differences between groups.\n",
        "\n",
        "4. **Risk of Type I Error**:\n",
        "   - Performing multiple t-tests increases the chance of Type I error, which is why ANOVA is preferred for comparing more than two groups. ANOVA controls for this by using a single test to compare all group means simultaneously.\n",
        "\n",
        "### **When to Use**:\n",
        "   - Use a **t-test** when comparing **two groups** (e.g., comparing the average heights of men and women).\n",
        "   - Use **ANOVA** when comparing **three or more groups** (e.g., comparing the average test scores of students in three different teaching methods).\n",
        "\n"
      ],
      "metadata": {
        "id": "fFmX-wv8mnLF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more  than two groups**\n",
        "\n",
        "## When and Why to Use a One-Way ANOVA Instead of Multiple t-Tests\n",
        "\n",
        "### **When to Use a One-Way ANOVA**:\n",
        "   - **One-Way ANOVA** is used when you want to compare the means of **three or more independent groups** on a single factor (independent variable).\n",
        "   - For example, if you're comparing the average test scores of students in three different teaching methods, a one-way ANOVA is appropriate because there are more than two groups.\n",
        "\n",
        "### **Why Not Use Multiple t-Tests?**:\n",
        "   - When you have more than two groups, you might be tempted to perform multiple t-tests (e.g., comparing group 1 vs group 2, group 1 vs group 3, and so on).\n",
        "   - However, this approach increases the risk of committing a **Type I error** (false positive), which occurs when you incorrectly reject the null hypothesis.\n",
        "   - Each t-test has a significance level, typically set at \\( \\alpha = 0.05 \\). If you perform multiple t-tests, the probability of making at least one Type I error increases with each additional test.\n",
        "\n",
        "### **Increased Risk of Type I Error**:\n",
        "   - For example, if you perform 3 t-tests at a significance level of 0.05, the probability of making at least one error is:\n",
        "$$\n",
        "1 - (1 - 0.05)^3 = 0.14 \\text{ or } 14\\%\n",
        "$$\n",
        "\n",
        "   - This means there is a 14% chance of incorrectly finding a significant difference, even if there is none.\n",
        "\n",
        "### **Why Use One-Way ANOVA?**:\n",
        "   - **One-Way ANOVA** controls for the increased risk of Type I error by using a **single test** to compare all group means simultaneously.\n",
        "   - It tests whether there are any statistically significant differences between the group means, without increasing the error rate like multiple t-tests do.\n",
        "   - If the ANOVA result is significant, you can then perform **post-hoc tests** (like Tukey’s test) to determine exactly which groups differ from each other.\n",
        "\n",
        "### **Summary**:\n",
        "   - Use **one-way ANOVA** instead of multiple t-tests when comparing **three or more groups** because it maintains the overall significance level (\\( \\alpha \\)) and avoids the inflated risk of Type I error.\n"
      ],
      "metadata": {
        "id": "kTGwMFhDn7vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic?\n",
        "\n",
        "## Variance Partitioning in ANOVA\n",
        "\n",
        "### **Understanding ANOVA**:\n",
        "ANOVA (Analysis of Variance) is a statistical method used to compare the means of three or more groups. It helps to determine whether there are significant differences among group means based on variance.\n",
        "\n",
        "### **Variance Partitioning**:\n",
        "In ANOVA, the total variance of the data is partitioned into two components:\n",
        "\n",
        "1. **Between-Group Variance**:\n",
        "   - This measures the variance among the group means. It reflects how much the means of different groups deviate from the overall mean of the data.\n",
        "   - Mathematically, it can be expressed as:\n",
        "   $$\n",
        "   \\text{SS}_{\\text{between}} = \\sum_{i=1}^{k} n_i (X_i - \\bar{X})^2\n",
        "   $$\n",
        "\n",
        "   Where:\n",
        "   - \\( n_i \\): Number of observations in group \\( i \\)\n",
        "   - \\( X_i \\): Mean of group \\( i \\)\n",
        "   - \\( \\bar{X} \\): Overall mean of all groups\n",
        "   - \\( k \\): Number of groups\n",
        "\n",
        "2. **Within-Group Variance**:\n",
        "   - This measures the variance within each group. It reflects how much individual observations within each group deviate from their respective group means.\n",
        "   - It can be expressed as:\n",
        "$$\n",
        "     \\text{SS}_{\\text{within}} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (X_{ij} - \\bar{X}_i)^2\n",
        "$$\n",
        "     Where:\n",
        "     - \\( X_{ij} \\) is the \\( j \\)-th observation in group \\( i \\),\n",
        "     - \\( \\bar{X}_i \\) is the mean of group \\( i \\),\n",
        "     - \\( n_i \\) is the number of observations in group \\( i \\),\n",
        "     - \\( k \\) is the number of groups.\n",
        "\n",
        "### **Total Variance**:\n",
        "The total variance (Total Sum of Squares) can be expressed as the sum of between-group variance and within-group variance:\n",
        "$$\n",
        "\\text{SS}_{\\text{total}} = \\text{SS}_{\\text{between}} + \\text{SS}_{\\text{within}}\n",
        "$$\n",
        "\n",
        "### **F-statistic Calculation**:\n",
        "The F-statistic is calculated by comparing the between-group variance to the within-group variance. It is expressed as:\n",
        "$$\n",
        "F = \\frac{\\text{MS}_{\\text{between}}}{\\text{MS}_{\\text{within}}}\n",
        "$$\n",
        "Where:\n",
        "\n",
        " (Mean Square Between) is calculated as:\n",
        "$$\n",
        "\\text{MS}_{\\text{between}} = \\frac{\\text{SS}_{\\text{between}}}{k - 1}\n",
        "$$\n",
        " (Mean Square Within) is calculated as:\n",
        "$$\n",
        "\\text{MS}_{\\text{within}} = \\frac{\\text{SS}_{\\text{within}}}{N - k}\n",
        "$$\n",
        "Where:\n",
        "- \\( N \\) is the total number of observations,\n",
        "- \\( k \\) is the number of groups.\n",
        "\n",
        "### **Conclusion**:\n",
        "The partitioning of variance in ANOVA into between-group and within-group variance allows us to assess whether the group means are significantly different from each other. A higher F-statistic indicates that the between-group variance is greater relative to the within-group variance, suggesting that the group means differ significantly.\n"
      ],
      "metadata": {
        "id": "y-A0Ya3ypv8i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?**\n",
        "\n",
        "# Comparison of Classical (Frequentist) and Bayesian Approaches to ANOVA\n",
        "\n",
        "## 1. Handling Uncertainty\n",
        "- **Classical (Frequentist) Approach**:\n",
        "  - Uncertainty is quantified using confidence intervals and p-values.\n",
        "  - Assumes a fixed parameter and derives probabilities based on the sampling distribution of the statistic.\n",
        "  - Relies on the notion of long-run frequency properties.\n",
        "  \n",
        "- **Bayesian Approach**:\n",
        "  - Incorporates prior beliefs about parameters, updating these beliefs with data to form a posterior distribution.\n",
        "  - Uncertainty is expressed through probability distributions rather than point estimates.\n",
        "  - Allows for a more intuitive interpretation of uncertainty, as it provides a distribution of possible values for parameters.\n",
        "\n",
        "## 2. Parameter Estimation\n",
        "- **Classical (Frequentist) Approach**:\n",
        "  - Parameters are estimated using point estimates (e.g., means, variances) derived from sample data.\n",
        "  - Focuses on estimating fixed parameters and uses methods like Maximum Likelihood Estimation (MLE).\n",
        "  - Does not incorporate prior information, relying solely on the data at hand.\n",
        "  \n",
        "- **Bayesian Approach**:\n",
        "  - Parameters are treated as random variables with their own distributions (priors).\n",
        "  - Uses Bayes' theorem to update beliefs about parameters as new data becomes available.\n",
        "  - Results in a posterior distribution that reflects both prior beliefs and the observed data, allowing for richer insights.\n",
        "\n",
        "## 3. Hypothesis Testing\n",
        "- **Classical (Frequentist) Approach**:\n",
        "  - Utilizes null hypothesis significance testing (NHST), where hypotheses are tested against a null hypothesis (e.g., no difference between means).\n",
        "  - P-values are calculated to determine whether to reject the null hypothesis based on a predetermined significance level (e.g., α = 0.05).\n",
        "  - A significant result indicates that the observed data would be very unlikely under the null hypothesis.\n",
        "\n",
        "- **Bayesian Approach**:\n",
        "  - Does not rely on null hypothesis testing; instead, it provides a direct probability of a hypothesis given the data.\n",
        "  - Can compare models or hypotheses using Bayes factors, which quantify the evidence for one model versus another.\n",
        "  - Allows for more flexible hypothesis testing by considering alternative hypotheses and incorporating prior information.\n",
        "\n",
        "## Key Differences Summary\n",
        "- **Uncertainty**: Frequentist approach uses confidence intervals and p-values; Bayesian approach uses probability distributions.\n",
        "- **Parameter Estimation**: Frequentist approach uses point estimates; Bayesian approach uses distributions to estimate parameters.\n",
        "- **Hypothesis Testing**: Frequentist approach focuses on NHST and p-values; Bayesian approach uses prior information and Bayes factors for hypothesis comparison.\n",
        "\n",
        "## Conclusion\n",
        "Both the classical and Bayesian approaches to ANOVA have their strengths and weaknesses. The choice between them often depends on the context of the analysis, the nature of the data, and the specific research questions being addressed. While the frequentist approach is widely used in traditional statistics, the Bayesian approach offers a more flexible and intuitive framework for incorporating prior knowledge and managing uncertainty.\n"
      ],
      "metadata": {
        "id": "Ymy-kR3IueMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **8. Question: You have two sets of data representing the incomes of two different professions**\n",
        "\n",
        " Profession A: [48, 52, 55, 60, 62 ]\n",
        "\n",
        " Profession B: [45, 50, 55, 52, 47]\n",
        "\n",
        "# Perform an F-test to determine if the variances of the two professions incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        " Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        " Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison"
      ],
      "metadata": {
        "id": "b_tTs1UEyKcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.stats as stats\n",
        "\n",
        "profession_a = [48, 52, 55, 60, 62]\n",
        "profession_b = [45, 50, 55, 52, 47]\n",
        "\n",
        "# variance\n",
        "var_a = np.var(profession_a, ddof=1)\n",
        "var_b = np.var(profession_b, ddof=1)\n",
        "\n",
        "# F - Statistics\n",
        "f_statistic = var_a / var_b\n",
        "\n",
        "df_a = len(profession_a) - 1\n",
        "df_b = len(profession_b) - 1\n",
        "\n",
        "p_value = 1 - stats.f.cdf(f_statistic, df_a, df_b)\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    conclusion = \"Reject the null hypothesis: the variances are significantly different.\"\n",
        "else:\n",
        "    conclusion = \"Fail to reject the null hypothesis: there is no significant difference in variances.\"\n",
        "\n",
        "print(f\"F-statistic: {f_statistic:.5f}\")\n",
        "print(f\"p-value: {p_value:.5f}\")\n",
        "print(conclusion)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3olyUoJ0xYHS",
        "outputId": "2e97ab70-73c4-4043-a186-fdd0a6e01386"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 2.08917\n",
            "p-value: 0.24652\n",
            "Fail to reject the null hypothesis: there is no significant difference in variances.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data**\n",
        "\n",
        " Region A: [160, 162, 165, 158, 164]\n",
        "\n",
        " Region B: [172, 175, 170, 168, 174]\n",
        "\n",
        " Region C: [180, 182, 179, 185, 183]\n",
        "\n",
        " Task: Write Python code to perform the one-way ANOVA and interpret the results\n",
        "\n",
        " Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value"
      ],
      "metadata": {
        "id": "pUYzBhWGz7G7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "region_a = [160, 162, 165, 158, 164]\n",
        "region_b = [172, 175, 170, 168, 174]\n",
        "region_c = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Conduct one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n",
        "\n",
        "print(f\"F-statistic: {f_statistic:.4f}\")\n",
        "print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "alpha = 0.05\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There are statistically significant differences in average heights between regions.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There are no statistically significant differences in average heights between regions.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ga-dQEGo0MzG",
        "outputId": "cd2a7a81-457d-4ee7-bcb4-fe8135e9054a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.8733\n",
            "P-value: 0.0000\n",
            "Reject the null hypothesis: There are statistically significant differences in average heights between regions.\n"
          ]
        }
      ]
    }
  ]
}